{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_channels(raw_df: pd.DataFrame, column_data_str: str, labels):\n",
    "    f, ax = plt.subplots(len(column_data_str), 1, figsize=(30, 3 * len(column_data_str)))\n",
    "    f.tight_layout(pad=2)\n",
    "    timeseries_len = raw_df.shape[0]\n",
    "    for index, column_name in enumerate(column_data_str):\n",
    "        ax[index].plot(range(timeseries_len), raw_df[column_name])\n",
    "        ax[index].set_title(f\"channel: {column_name}\", fontsize=20)\n",
    "\n",
    "        height_line = 1\n",
    "        ax[index].fill_between(range(timeseries_len), 0, height_line, where=labels > 0, color='red', alpha=0.2, transform=ax[index].get_xaxis_transform())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HASC-2011 dataset preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw shape: (39397, 4), label shape: (38, 3)\n",
      "Raw activity data\n",
      "          time         x         y         z\n",
      "0  5015.672119  0.115128 -0.988739 -0.090057\n",
      "1  5015.687371  0.110138 -0.986694 -0.086288\n",
      "2  5015.704061  0.116180 -0.991669 -0.091003\n",
      "3  5015.715389  0.112259 -0.989670 -0.091095\n",
      "4  5015.726564  0.114349 -0.989746 -0.097855\n",
      "\n",
      "raw label data\n",
      "      start       end                    label\n",
      "0  5071.934       NaN       move;escalator;B2F\n",
      "1  5098.502  5126.499         escalatorUp;stay\n",
      "2  5126.970       NaN           move;floor;B1F\n",
      "3  5127.665  5143.411  walk;floor;B1F;steps;22\n",
      "4  5147.988       NaN       move;escalator;B1F\n"
     ]
    }
   ],
   "source": [
    "raw_file = './data/hasc-111018-165936-acc.csv'\n",
    "raw_label = './data/hasc-111018-165936-acc.label'\n",
    "\n",
    "raw_df = pd.read_csv(raw_file, header=None)\n",
    "raw_df.columns = ['time', 'x', 'y', 'z']\n",
    "raw_label_df = pd.read_csv(raw_label, skiprows=1, header=None)\n",
    "raw_label_df.columns = ['start', 'end', 'label']\n",
    "print(f'raw shape: {raw_df.shape}, label shape: {raw_label_df.shape}')\n",
    "print('Raw activity data')\n",
    "print(raw_df.head())\n",
    "print('\\nraw label data')\n",
    "print(raw_label_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index         time         x         y         z\n",
      "0      0  5015.672119  0.115128 -0.988739 -0.090057\n",
      "1      1  5015.687371  0.110138 -0.986694 -0.086288\n",
      "2      2  5015.704061  0.116180 -0.991669 -0.091003\n",
      "3      3  5015.715389  0.112259 -0.989670 -0.091095\n",
      "4      4  5015.726564  0.114349 -0.989746 -0.097855\n"
     ]
    }
   ],
   "source": [
    "# add index column\n",
    "raw_df = raw_df.reset_index()\n",
    "print(raw_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.999485\n",
       "1    0.996565\n",
       "2    1.002590\n",
       "3    1.000174\n",
       "4    1.001124\n",
       "Name: l2_norm, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df['l2_norm'] = np.sqrt(np.square(raw_df[['x', 'y', 'z']]).sum(axis=1))\n",
    "raw_df['l2_norm'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df[['l2_norm']].to_csv('./data/preprocess/hasc_l2_norm.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create breakpoints index label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_time = raw_df['time'].to_numpy()\n",
    "raw_label_start = raw_label_df['start'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 40, value: [3959, 5305, 7812, 10325, 10565, 13084, 14136, 14184, 14411, 14931, 18428, 19901, 21086, 21538, 23834, 24236, 24469, 24818, 24884, 25214, 25597, 25710, 25780, 26510, 27408, 27431, 28116, 28396, 29188, 30016, 30072, 30407, 30769, 30858, 33567, 34033, 34265, 35529, 37511, 37787]\n"
     ]
    }
   ],
   "source": [
    "breakpoints_index = []\n",
    "start_index = 0\n",
    "for i in range(len(raw_time)):\n",
    "    if start_index < len(raw_label_start) and raw_time[i] > raw_label_start[start_index]:\n",
    "        breakpoints_index.append(i)\n",
    "        start_index += 1\n",
    "print(f'len: {len(breakpoints_index)}, value: {breakpoints_index}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "np.savetxt('./data/preprocess/hasc_label_index.txt', breakpoints_index, fmt='%i')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG dataset preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Minh Nhat\\\\Downloads\\\\DCU\\\\Practicum\\\\TIRE-custom'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.abspath('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Minh Nhat\\Downloads\\DCU\\Practicum\\TIRE-custom\\../Data/grasp-and-lift-eeg-detection/train/\n",
      "file data format: subj1_series2_data.csv\n",
      "file events format: subj1_series2_events.csv\n"
     ]
    }
   ],
   "source": [
    "# dirname = os.path.dirname(__file__) # it's not working with ipynb\n",
    "dirname = os.path.abspath('')\n",
    "eeg_training_data_folder = os.path.join(dirname, '../Data/grasp-and-lift-eeg-detection/train/')\n",
    "print(eeg_training_data_folder)\n",
    "\n",
    "file_data_format = 'subj{}_series{}_data.csv'\n",
    "file_events_format = 'subj{}_series{}_events.csv'\n",
    "subject = 1\n",
    "series = 2\n",
    "print(f'file data format: {file_data_format.format(subject, series)}')\n",
    "print(f'file events format: {file_events_format.format(subject, series)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def create_files_by_template(base_folder: str, template: str, subjects: iter, series: int):\n",
    "    result = []\n",
    "    for subject in subjects:\n",
    "        file_name = template.format(subject, series)\n",
    "        result.append(os.path.join(base_folder, file_name))\n",
    "    return result \n",
    "\n",
    "def concat_files_by_row(files: List[str]):\n",
    "    def read_file_df(f: str):\n",
    "        df = pd.read_csv(f)\n",
    "        return df\n",
    "    frames = [read_file_df(f) for f in files ]\n",
    "    result = pd.concat(frames)\n",
    "    return result \n",
    "\n",
    "# print(f'test create_files_by_template')\n",
    "# list_file_label = create_files_by_template(eeg_training_data_folder, file_events_format, subjects=range(1, 3), series=1)\n",
    "# print(list_file_label)\n",
    "# combine_label_df = concat_files_by_row(list_file_label)\n",
    "# combine_label_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(series: int):\n",
    "    # create database by series. \n",
    "    # training: subject 1-> 8 / validation: 9, 10 / testing: 11, 12\n",
    "    configs = [('training', range(1, 9)), ('validation', range(9, 11)), ('testing', range(11, 13))]\n",
    "\n",
    "    if os.path.exists('./data/eeg_grasp_and_lift/') == False:\n",
    "        os.makedirs('./data/eeg_grasp_and_lift/')\n",
    "        \n",
    "    for config in configs:\n",
    "        print(f'type: {config[0]}')\n",
    "        list_file_data = create_files_by_template(eeg_training_data_folder, file_data_format, subjects=config[1], series=series)\n",
    "        combine_data_df = concat_files_by_row(list_file_data)\n",
    "        combine_data_df.to_csv(f'./data/eeg_grasp_and_lift/dataset{series}_{config[0]}_data.csv', index=False)\n",
    "\n",
    "        list_file_label = create_files_by_template(eeg_training_data_folder, file_events_format, subjects=config[1], series=series)\n",
    "        combine_label_df = concat_files_by_row(list_file_label)\n",
    "        combine_label_df.to_csv(f'./data/eeg_grasp_and_lift/dataset{series}_{config[0]}_label.csv', index=False)\n",
    "\n",
    "# test create dataset 1\n",
    "# create_dataset(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: training\n",
      "type: validation\n",
      "type: testing\n",
      "type: training\n",
      "type: validation\n",
      "type: testing\n",
      "type: training\n",
      "type: validation\n",
      "type: testing\n",
      "type: training\n",
      "type: validation\n",
      "type: testing\n",
      "type: training\n",
      "type: validation\n",
      "type: testing\n",
      "type: training\n",
      "type: validation\n",
      "type: testing\n"
     ]
    }
   ],
   "source": [
    "for series in range(3, 9):\n",
    "    create_dataset(series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generated dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jump mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simulate \n",
    "import utils\n",
    "\n",
    "utils.setup_random_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For formulation, check the TIRE paper at page 5\n",
    "Target: examine what data circumstances DMD should be chosen over L2 ?\n",
    "- [Done] Case 1: For jumpmean \n",
    "    - 1 real jump mean\n",
    "    - 2 noise channels (still use AutoRegressive but with constant mean and variance). \n",
    "- Case 2: For jumpman\n",
    "    - 1 real jump mean \n",
    "    - 2 noise channels, but use Gausian noise \n",
    "- Case 3: Channels are nearly the same in distribution shape, but different in magnitude \n",
    "    - Case 3.1: 3 channels jump mean \n",
    "    - Case 3.2: 3 channels Gaussian \n",
    "- Case 4: Mean and variance changed \n",
    "    - 1 channel mean \n",
    "    - 1 channel variance \n",
    "    - 1 channels noise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## case 1 \n",
    "def generate_jumpingmean_ar_noise(nr_cp=49, delta_t_cp = 100, delta_t_cp_std = 10, window_size=20):\n",
    "    \"\"\"\n",
    "    Generates one instance of a jumping mean time series, together with the corresponding windows and parameters\n",
    "    \"\"\"\n",
    "    mu = np.zeros((nr_cp,))\n",
    "    parameters_jumpingmean = []\n",
    "    for n in range(1,nr_cp):\n",
    "        mu[n] = mu[n-1] + n / 16 #np.random.randn()*np.sqrt(delta_t_cp_std**3)\n",
    "    # print(f'mu: {mu}')\n",
    "    for n in range(nr_cp):\n",
    "        nr = int(delta_t_cp+ np.random.randn()*np.sqrt(delta_t_cp_std))\n",
    "        parameters_jumpingmean.extend(mu[n]*np.ones((nr,)))\n",
    "    \n",
    "    parameters_jumpingmean = np.array([parameters_jumpingmean]).T\n",
    "\n",
    "    channels = 3\n",
    "    ts_length = len(parameters_jumpingmean)\n",
    "    all_timeseries = []\n",
    "    for channel in range(channels):\n",
    "        timeseries = np.zeros((ts_length))\n",
    "        for i in range(2,ts_length):\n",
    "            if channel == 0:\n",
    "            #print(ar2(timeseries[i-1],timeseries[i-2], 0.6,-0.5, parameters_jumpingmean[i], 1.5))\n",
    "                timeseries[i] = simulate.ar2(timeseries[i-1],timeseries[i-2], 0.6,-0.5, parameters_jumpingmean[i], 1.5)\n",
    "            elif channel == 1:\n",
    "                timeseries[i] = simulate.ar2(timeseries[i-1],timeseries[i-2], 0.9,-0.7, -1, 5.5)\n",
    "            else:\n",
    "                timeseries[i] = simulate.ar2(timeseries[i-1],timeseries[i-2], 0.6,-0.4, 0, 3.5)\n",
    "        \n",
    "        all_timeseries.append(timeseries)\n",
    "    \n",
    "    breakpoints = utils.parameters_to_cps(parameters_jumpingmean, window_size) # len(breakpoints) = len(timeseries) - 2*window_size + 1\n",
    "    full_breakpoints =  np.concatenate([[0] * (window_size - 1), breakpoints , [0] * window_size])\n",
    "\n",
    "    b_index = np.where(full_breakpoints > 0)[0]\n",
    "    full_breakpoints[b_index] = [1] * len(b_index)\n",
    "    \n",
    "    return np.array(all_timeseries), full_breakpoints\n",
    "\n",
    "### case 2 \n",
    "def generate_jumpingmean_gaussian_noise(nr_cp=49, delta_t_cp = 100, delta_t_cp_std = 10, window_size=20):\n",
    "    \"\"\"\n",
    "    Generates one instance of a jumping mean time series, together with the corresponding windows and parameters\n",
    "    \"\"\"\n",
    "    mu = np.zeros((nr_cp,))\n",
    "    parameters_jumpingmean = []\n",
    "    for n in range(1,nr_cp):\n",
    "        mu[n] = mu[n-1] + n / 16 #np.random.randn()*np.sqrt(delta_t_cp_std**3)\n",
    "    # print(f'mu: {mu}')\n",
    "    for n in range(nr_cp):\n",
    "        nr = int(delta_t_cp+ np.random.randn()*np.sqrt(delta_t_cp_std))\n",
    "        parameters_jumpingmean.extend(mu[n]*np.ones((nr,)))\n",
    "    \n",
    "    parameters_jumpingmean = np.array([parameters_jumpingmean]).T\n",
    "\n",
    "    channels = 3\n",
    "    ts_length = len(parameters_jumpingmean)\n",
    "    all_timeseries = []\n",
    "    for channel in range(channels):\n",
    "        timeseries = np.zeros((ts_length))\n",
    "        for i in range(2,ts_length):\n",
    "            if channel == 0:\n",
    "            #print(ar2(timeseries[i-1],timeseries[i-2], 0.6,-0.5, parameters_jumpingmean[i], 1.5))\n",
    "                timeseries[i] = simulate.ar2(timeseries[i-1],timeseries[i-2], 0.6,-0.5, parameters_jumpingmean[i], 1.5)\n",
    "            elif channel == 1:\n",
    "                timeseries[i] = np.random.normal(-1, 5.5)\n",
    "            else:\n",
    "                timeseries[i] = np.random.normal(0, 3.5)\n",
    "        \n",
    "        all_timeseries.append(timeseries)\n",
    "    \n",
    "    breakpoints = utils.parameters_to_cps(parameters_jumpingmean, window_size) # len(breakpoints) = len(timeseries) - 2*window_size + 1\n",
    "    full_breakpoints =  np.concatenate([[0] * (window_size - 1), breakpoints , [0] * window_size])\n",
    "\n",
    "    b_index = np.where(full_breakpoints > 0)[0]\n",
    "    full_breakpoints[b_index] = [1] * len(b_index)\n",
    "    \n",
    "    return np.array(all_timeseries), full_breakpoints\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 3: Channels are nearly the same in distribution shape, but different in magnitude \n",
    "# Case 3.1: 3 channels jump mean \n",
    "def generate_jumpingmean_diff_magnitude(nr_cp=49, delta_t_cp = 100, delta_t_cp_std = 10, window_size=20):\n",
    "    \"\"\"\n",
    "    Generates one instance of a jumping mean time series, together with the corresponding windows and parameters\n",
    "    \"\"\"\n",
    "    mu = np.zeros((nr_cp,))\n",
    "    parameters_jumpingmean = []\n",
    "    for n in range(1,nr_cp):\n",
    "        mu[n] = mu[n-1] + n / 16 #np.random.randn()*np.sqrt(delta_t_cp_std**3)\n",
    "    # print(f'mu: {mu}')\n",
    "    for n in range(nr_cp):\n",
    "        nr = int(delta_t_cp+ np.random.randn()*np.sqrt(delta_t_cp_std))\n",
    "        parameters_jumpingmean.extend(mu[n]*np.ones((nr,)))\n",
    "    \n",
    "    parameters_jumpingmean = np.array([parameters_jumpingmean]).T\n",
    "\n",
    "    channels = 3\n",
    "    ts_length = len(parameters_jumpingmean)\n",
    "    all_timeseries = []\n",
    "    for channel in range(channels):\n",
    "        timeseries = np.zeros((ts_length))\n",
    "        for i in range(2,ts_length):\n",
    "            if channel == 0:\n",
    "            #print(ar2(timeseries[i-1],timeseries[i-2], 0.6,-0.5, parameters_jumpingmean[i], 1.5))\n",
    "                timeseries[i] = simulate.ar2(timeseries[i-1],timeseries[i-2], 0.6,-0.5, parameters_jumpingmean[i], 1.5)\n",
    "            elif channel == 1:\n",
    "                timeseries[i] = simulate.ar2(timeseries[i-1],timeseries[i-2], 0.9,-0.7, parameters_jumpingmean[i], 5.5)\n",
    "            else:\n",
    "                timeseries[i] = simulate.ar2(timeseries[i-1],timeseries[i-2], 0.8,-0.1, parameters_jumpingmean[i], 3.5)\n",
    "        \n",
    "        all_timeseries.append(timeseries)\n",
    "    \n",
    "    breakpoints = utils.parameters_to_cps(parameters_jumpingmean, window_size) # len(breakpoints) = len(timeseries) - 2*window_size + 1\n",
    "    full_breakpoints =  np.concatenate([[0] * (window_size - 1), breakpoints , [0] * window_size])\n",
    "\n",
    "    b_index = np.where(full_breakpoints > 0)[0]\n",
    "    full_breakpoints[b_index] = [1] * len(b_index)\n",
    "    \n",
    "    return np.array(all_timeseries), full_breakpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saving_single_generate_dataset(timeseries, breakpoints, dataset_number: int, datatype: str, folder_prefix: str):\n",
    "    utils.create_folder_if_not_exist(f'./data-gen/{folder_prefix}')\n",
    "\n",
    "    timeseries_df = pd.DataFrame(timeseries.T)\n",
    "    timeseries_df.columns = [f'col_{col}' for col in timeseries_df.columns]\n",
    "    timeseries_df.to_csv(f'./data-gen/{folder_prefix}/{folder_prefix}-dataset{dataset_number}-{datatype}-data.csv', index=False)\n",
    "    \n",
    "\n",
    "    breakpoints_df = pd.DataFrame(breakpoints)\n",
    "    breakpoints_df.columns = [f'col_{col}' for col in breakpoints_df.columns]\n",
    "    breakpoints_df.to_csv(f'./data-gen/{folder_prefix}/{folder_prefix}-dataset{dataset_number}-{datatype}-label.csv', index=False)\n",
    "\n",
    "'''\n",
    "generated_type:\n",
    "case 1: jumpmean_ar\n",
    "case 2: jumpmean_gauss\n",
    "case 3.1: jumpmean_same_dis\n",
    "case 3.2: gauss_same_dis\n",
    "case 4: jummpmean_scalevariance\n",
    "'''\n",
    "def saving_generate_datasets(generated_type: str):\n",
    "    # if generated_type == 'jumpmean_ar' or generated_type == 'case1':\n",
    "    #     generate_dataset_f = generate_jumpingmean_ar_noise\n",
    "    #     folder_prefix = 'jumpmean_ar'\n",
    "    if generated_type == 'jumpmean_gauss' or generated_type == 'case2':\n",
    "        generate_dataset_f = generate_jumpingmean_gaussian_noise\n",
    "        folder_prefix = 'jumpmean-gauss' # for folder convention \n",
    "    else:\n",
    "        print('Please check generated_type parameter')\n",
    "        return \n",
    "    for dataset_number in range(1, 11):\n",
    "        for t in [('training', 49), ('validation', 49), ('testing', 49)]:\n",
    "\n",
    "            timeseries, breakpoints = generate_dataset_f(nr_cp = t[1], window_size=20) # 20 for jump mean\n",
    "            saving_single_generate_dataset(timeseries, breakpoints, dataset_number, t[0], folder_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_generate_datasets('jumpmean_gauss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data-gen/jumpmean-gauss/jumpmean-gauss-dataset1-training-data.csv'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_prefix = '../data-gen/jumpmean-gauss/jumpmean-gauss'\n",
    "dataset_number = 1 \n",
    "f'{folder_prefix}-dataset{dataset_number}-training-data.csv'"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1e259f8989b5c9c45559a1752ef2f72f727f33e7b4cd49fa4d8dfdfbce9ad9e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
